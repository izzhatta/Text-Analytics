{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6d3771-8c47-4d0a-97be-3d0631fbd990",
   "metadata": {},
   "source": [
    "# CISB5123 Text Analytics\n",
    "## Lab Assignment 3 - Topic Modeling\n",
    "\n",
    "**Group members:**\n",
    "1. IZZAT HATTA BIN AZIZI SW01082390\n",
    "2. MUHAMMAD HAKIMI BIN AZIZI SW01082355\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b9fa69-51e9-4c94-82ea-35db9c93a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bebc2f00-c5b5-4291-b521-69641298adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Read the dataset\n",
    "df = pd.read_csv('Dataset/news_dataset.csv')\n",
    "df = df[['text']].dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f562345-471b-4a67-8a82-5d41f0c4645f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download nltk resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create a set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize a WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92545797-cc73-4c29-8abc-2b801f73108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample FIRST (important)\n",
    "df_sample = df.sample(n=200, random_state=42)\n",
    "\n",
    "# Now safely preprocess the sampled text only\n",
    "sampled_docs = [preprocess_text(doc) for doc in df_sample['text']]\n",
    "\n",
    "# Proceed as usual\n",
    "dictionary = corpora.Dictionary(sampled_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in sampled_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4251659-fa95-4640-b742-417b23720de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build the LDA Model\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=dictionary,\n",
    "                     num_topics=4,\n",
    "                     random_state=42,\n",
    "                     update_every=1,\n",
    "                     chunksize=20,\n",
    "                     passes=3,\n",
    "                     alpha='auto',\n",
    "                     per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e2c684-0bf2-4fa9-ad20-177ec0656b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate the LDA model using Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=sampled_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef30166d-9138-4658-b8a4-da38487e9124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Topics Found ===\n",
      "(0, '0.010*\"law\" + 0.008*\"technology\" + 0.008*\"encryption\" + 0.007*\"program\" + 0.006*\"new\" + 0.006*\"one\" + 0.006*\"muslim\" + 0.005*\"safety\" + 0.005*\"device\" + 0.005*\"case\"')\n",
      "(1, '0.015*\"state\" + 0.008*\"people\" + 0.008*\"one\" + 0.007*\"u\" + 0.007*\"like\" + 0.006*\"time\" + 0.006*\"jew\" + 0.006*\"would\" + 0.005*\"year\" + 0.005*\"right\"')\n",
      "(2, '0.013*\"health\" + 0.012*\"age\" + 0.009*\"medical\" + 0.007*\"disease\" + 0.007*\"arizona\" + 0.006*\"year\" + 0.005*\"among\" + 0.005*\"icsucieduincominggeodegif\" + 0.005*\"problem\" + 0.004*\"xtermmap\"')\n",
      "(3, '0.014*\"armenian\" + 0.013*\"child\" + 0.012*\"use\" + 0.007*\"number\" + 0.007*\"russian\" + 0.006*\"gonorrhea\" + 0.006*\"increased\" + 0.005*\"reported\" + 0.005*\"rate\" + 0.005*\"cdc\"')\n",
      "\n",
      "Coherence Score: 0.3980\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Interpret the topics\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "\n",
    "# Display topics and coherence score\n",
    "print(\"\\n=== Topics Found ===\")\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "print(f\"\\nCoherence Score: {coherence_lda:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711e757-56f0-4943-b70f-52e036ffaf61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125b541-07e0-48b4-8bd3-efc29e36a67e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
